{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2abd58e-9c1e-4b47-aab5-3ee3ced13c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This python script for NSRP based rainfall simulation can be grouped into 5 stages\n",
    "\n",
    "## Stage-1: Data/Time series preparation which includes computation of rainfall aggregations, quantifying missing data and \n",
    "## reducing the influence of outliers\n",
    "\n",
    "## Stage-2: In this stage we compute reference statistics obtained from the time series in Stage-1. These statistics to name a few\n",
    "## are mean, variance, skewness, autocorrelation, dry proportions\n",
    "\n",
    "## Stage-3: The output from Stage-2 which is the reference statistics is used as input and parameters of NSRP model are computed \n",
    "\n",
    "## Stage-4: In this stage we use the parameters computed from Stage-3 to simulate realizations of rainfall for the given rain-gauge\n",
    "\n",
    "## Stage-5: Finally, in this stage we put the functions from Stage-1 through Stage-4 to execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f963b3e2-52eb-4bdd-aeb3-9ba858bafe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "import calendar\n",
    "from scipy.optimize import differential_evolution\n",
    "from multiprocessing import Pool\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from scipy.stats import genextreme as gev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "279e98ac-d169-41fe-8c90-edbc7ea2cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage-1: Data/time series preparation stage begins from here ##\n",
    "def prepare_point_timeseries(df, season_definitions, completeness_threshold, durations, outlier_method,\n",
    "        maximum_relative_difference, maximum_alterations,):\n",
    "    \"\"\"\n",
    "    Prepare point timeseries for analysis.\n",
    "\n",
    "    Steps are: (1) subset on reference calculation period, (2) define seasons for grouping, (3) applying any trimming\n",
    "    or clipping to reduce the influence of outliers, and (4) aggregating timeseries to required durations.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check valid or nan  # TODO: Revisit if this function gets used for non-precipitation variables\n",
    "    df.loc[df['value'] < 0.0] = np.nan\n",
    "\n",
    "    # Apply season definitions and make a running UID for season that goes up by one at each change in season\n",
    "    # through the time series. Season definitions are needed to identify season completeness but also to apply\n",
    "    # trimming or clipping\n",
    "    df['season'] = df.index.month.map(season_definitions)\n",
    "    df['season_uid'] = df['season'].ne(df['season'].shift()).cumsum()\n",
    "\n",
    "    # Mask periods not meeting data completeness threshold (close approximation). There is an assumption of at\n",
    "    # least one complete version of each season in dataframe (where complete means that nans may be present - i.e.\n",
    "    # fine unless only a very short (< 1 year) record is passed in)\n",
    "    if df['value'].isnull().any():\n",
    "        df['season_count'] = df.groupby('season_uid')['value'].transform('count')\n",
    "        df['season_size'] = df.groupby('season_uid')['value'].transform('size')\n",
    "        df['season_size'] = df.groupby('season')['season_size'].transform('median')\n",
    "        df['completeness'] = df['season_count'] / df['season_size'] * 100.0\n",
    "        df['completeness'] = np.where(df['completeness'] > 100.0, 100.0, df['completeness'])\n",
    "        df.loc[df['completeness'] < completeness_threshold, 'value'] = np.nan\n",
    "        df = df.loc[:, ['season', 'value']]\n",
    "\n",
    "    # Apply trimming or clipping season-wise\n",
    "    if outlier_method == 'trim':\n",
    "        df['value'] = df.groupby('season')['value'].transform(\n",
    "            trim_array(maximum_relative_difference, maximum_alterations)\n",
    "        )\n",
    "    elif outlier_method == 'clip':\n",
    "        df['value'] = df.groupby('season')['value'].transform(\n",
    "            clip_array(maximum_relative_difference, maximum_alterations)\n",
    "        )\n",
    "\n",
    "    # Find timestep and convert from datetime to period index if needed\n",
    "    if not isinstance(df.index, pd.PeriodIndex):\n",
    "        datetime_difference = df.index[1] - df.index[0]\n",
    "    else:\n",
    "        datetime_difference = df.index[1].to_timestamp() - df.index[0].to_timestamp()\n",
    "    timestep_length = int(datetime_difference.days * 24) + int(datetime_difference.seconds / 3600)  # hours\n",
    "    period = str(timestep_length) + 'H'  # TODO: Sort out sub-hourly timestep\n",
    "    if not isinstance(df.index, pd.PeriodIndex):\n",
    "        df = df.to_period(period)\n",
    "\n",
    "    # TODO: More efficient approach would be to use successive durations in aggregation\n",
    "    # - e.g use 1hr to get 3hr, but then use 3hr to get 6hr, 6hr to get 12hr, etc\n",
    "    # - only works if there is a neat division, otherwise need to go back to e.g. 1hr\n",
    "\n",
    "    # Prepare order to process durations in, so that long durations can be calculated from daily rather than hourly\n",
    "    # durations (as faster)\n",
    "    duration_hours = []\n",
    "    for duration in durations:\n",
    "        duration_units = duration[-1]\n",
    "        if duration_units == 'H':\n",
    "            duration_hours.append(int(duration[:-1]))\n",
    "        elif duration_units == 'D':\n",
    "            duration_hours.append(int(duration[:-1]) * 24)\n",
    "        elif duration_units == 'M':\n",
    "            duration_hours.append(31 * 24)\n",
    "    duration_hours = np.asarray(duration_hours)\n",
    "    sorted_durations = np.asarray(durations)[np.argsort(duration_hours)]\n",
    "\n",
    "    # Aggregate timeseries to required durations\n",
    "    dfs = {}\n",
    "    for duration in sorted_durations:\n",
    "        # resample_code = str(int(duration)) + 'H'  # TODO: Check/add sub-hourly\n",
    "        resample_code = duration\n",
    "        duration_units = duration[-1]\n",
    "        if duration_units == 'H':\n",
    "            duration_hours = int(duration[:-1])\n",
    "        elif duration_units == 'D':\n",
    "            duration_hours = int(duration[:-1]) * 24\n",
    "        elif duration_units == 'M':\n",
    "            duration_hours = 31 * 24\n",
    "\n",
    "        # Final day needed for a given aggregation\n",
    "        # - relies on multiples of one day if duration exceeds 24 hours\n",
    "        # - constrained to monthly\n",
    "        # - maximum duration of 28 days(?)\n",
    "        if duration_hours > 24:\n",
    "            duration_days = int(duration_hours / 24)\n",
    "\n",
    "            # Interim aggregation to daily to see if it speeds things up\n",
    "            if '24H' in durations:\n",
    "                df1 = dfs['24H'].copy()\n",
    "            elif '1D' in durations:\n",
    "                df1 = dfs['1D'].copy()\n",
    "\n",
    "            n_groups = int(np.ceil(31 / duration_days))\n",
    "            df1['group'] = -1\n",
    "            for group in range(n_groups):\n",
    "                if duration_units != 'M':\n",
    "                    df1['group'] = np.where(df1.index.day >= group * duration_days + 1, group, df1['group'])\n",
    "                else:\n",
    "                    # df1['month'] = df1.index.month\n",
    "                    # df1['group'] = df1['month'].ne(df1['month'].shift()).cumsum()\n",
    "                    # df1.drop(columns=['month'], inplace=True)\n",
    "                    df1['group'] = 0\n",
    "\n",
    "            # df1 = df.groupby([df.index.year, df.index.month, 'group'])['value'].agg(['sum', 'count'])\n",
    "            df1 = df1.groupby([df1.index.year, df1.index.month, 'group'])['value'].agg(['sum', 'count'])\n",
    "            if df1.index.names[0] == 'datetime':  # !221025 - for dfs coming from shuffling (fitting delta)\n",
    "                df1.index.rename(['level_0', 'level_1', 'group'], inplace=True)\n",
    "            df1.reset_index(inplace=True)\n",
    "            df1['day'] = df1['group'] * duration_days + 1\n",
    "            df1.rename(columns={'level_0': 'year', 'level_1': 'month'}, inplace=True)\n",
    "            df1['datetime'] = pd.to_datetime(df1[['year', 'month', 'day']])\n",
    "            df1.drop(columns=['year', 'month', 'day', 'group'], inplace=True)\n",
    "            df1.set_index('datetime', inplace=True)\n",
    "            # print(df1)\n",
    "        else:\n",
    "            df1 = df['value'].resample(resample_code, closed='left', label='left').agg(['sum', 'count'])\n",
    "            \n",
    "        # Remove data below a duration-dependent completeness\n",
    "        if duration_hours <= 24:  # TODO: Remove hardcoding of timestep requiring complete data and completeness threshold?\n",
    "            expected_count = int(duration_hours / timestep_length)\n",
    "        else:\n",
    "            expected_count = ((duration_hours / timestep_length) / 24) * 0.9  # TODO: Remove hardcoding - user option\n",
    "        # df1.values[df2.values < expected_count] = np.nan  # duration\n",
    "        df1.rename(columns={'sum': 'value'}, inplace=True)\n",
    "        df1.loc[df1['count'] < expected_count, 'value'] = np.nan\n",
    "        df1.drop(columns=['count'], inplace=True)\n",
    "        df1.sort_index(inplace=True)\n",
    "        # df1.drop(columns=['level_0'], inplace=True)\n",
    "\n",
    "        df1['season'] = df1.index.month.map(season_definitions)\n",
    "\n",
    "        dfs[duration] = df1\n",
    "        dfs[duration] = dfs[duration][dfs[duration]['value'].notnull()]\n",
    "\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def trim_array(max_relative_difference, max_removals):\n",
    "    def f(x):\n",
    "        y = x.copy()\n",
    "        removals = 0\n",
    "        while True:\n",
    "            y_max = np.max(y)\n",
    "            y_max_count = np.sum(y == y_max)\n",
    "            y_next_largest = np.max(y[y < y_max])\n",
    "            if y_max / y_next_largest > max_relative_difference:\n",
    "                if removals + y_max_count <= max_removals:\n",
    "                    y = y[y < y_max]\n",
    "                    removals += y_max_count\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        return y  # , removals\n",
    "    return f\n",
    "\n",
    "\n",
    "def clip_array(max_relative_difference, max_clips):\n",
    "    # - assuming working with zero-bounded values\n",
    "    def f(x):\n",
    "        y = x.copy()\n",
    "        clips = 0\n",
    "        clip_flag = -999\n",
    "        while True:\n",
    "            y_max = np.max(y)\n",
    "            y_max_count = np.sum(y == y_max)\n",
    "            y_next_largest = np.max(y[y < y_max])\n",
    "            if y_max / y_next_largest > max_relative_difference:\n",
    "                if clips + y_max_count <= max_clips:\n",
    "                    y[y == y_max] = clip_flag\n",
    "                    clips += y_max_count\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        y[y == clip_flag] = np.max(y)\n",
    "        return y  # , clips\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e0b2bbb-51ac-41f7-8a61-fe4e70313be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage-2: We get reference statistics from this stage and \n",
    "## data preparation stage ends here \n",
    "\n",
    "dc = {\n",
    "        1: {'weight': 1.0, 'duration': '1H', 'name': 'variance'},\n",
    "        2: {'weight': 2.0, 'duration': '1H', 'name': 'skewness'},\n",
    "        3: {'weight': 7.0, 'duration': '1H', 'name': 'probability_dry_0.2mm', 'threshold': 0.2},\n",
    "        4: {'weight': 6.0, 'duration': '24H', 'name': 'mean'},\n",
    "        5: {'weight': 2.0, 'duration': '24H', 'name': 'variance'},\n",
    "        6: {'weight': 3.0, 'duration': '24H', 'name': 'skewness'},\n",
    "        7: {'weight': 7.0, 'duration': '24H', 'name': 'probability_dry_0.2mm', 'threshold': 0.2},\n",
    "        8: {'weight': 6.0, 'duration': '24H', 'name': 'autocorrelation_lag1', 'lag': 1},\n",
    "        9: {'weight': 3.0, 'duration': '72H', 'name': 'variance'},\n",
    "        10: {'weight': 0.0, 'duration': '1M', 'name': 'variance'},\n",
    "    }\n",
    "id_name = 'statistic_id'\n",
    "non_id_columns = ['name', 'duration', 'lag', 'threshold', 'weight']\n",
    "\n",
    "\n",
    "def nested_dictionary_to_dataframe(dc, id_name, non_id_columns):\n",
    "    ids = sorted(list(dc.keys()))\n",
    "    data = {}\n",
    "    for non_id_column in non_id_columns:\n",
    "        data[non_id_column] = []\n",
    "        for id_ in ids:\n",
    "            values = dc[id_]\n",
    "            data[non_id_column].append(\n",
    "                values[non_id_column] if non_id_column in values.keys() else 'NA'\n",
    "            )\n",
    "    dc1 = {}\n",
    "    dc1[id_name] = ids\n",
    "    for non_id_column in non_id_columns:\n",
    "        dc1[non_id_column] = data[non_id_column]\n",
    "    df = pd.DataFrame(dc1)\n",
    "    return df\n",
    "\n",
    "statistic_definitions = nested_dictionary_to_dataframe(dc, id_name, non_id_columns)\n",
    "\n",
    "def GetMonthStats(ListofDFs):\n",
    "    '''\n",
    "    This function requires outputs from the functions PrepareTimeSeriesPoint\n",
    "    and nested_dictionary_to_dataframe\n",
    "    '''\n",
    "    statistic_definitions = nested_dictionary_to_dataframe(dc, id_name, non_id_columns)\n",
    "    statistic_definitions[statistic_definitions['duration']=='1H'].name\n",
    "    statistic_definitions[statistic_definitions['duration']=='24H'].name\n",
    "    statistic_definitions[statistic_definitions['duration']=='72H'].name\n",
    "    statistic_definitions[statistic_definitions['duration']=='1M'].name\n",
    "\n",
    "    ListofDFs['1H']['Month'] = [ListofDFs['1H'].index[i].month for i in np.arange(0,ListofDFs['1H'].shape[0],1)]\n",
    "    ListofDFs['1H']['Year'] = [ListofDFs['1H'].index[i].year for i in np.arange(0,ListofDFs['1H'].shape[0],1)]\n",
    "\n",
    "    ListofDFs['24H']['Month'] = [ListofDFs['24H'].index[i].month for i in np.arange(0,ListofDFs['24H'].shape[0],1)]\n",
    "    ListofDFs['24H']['Year'] = [ListofDFs['24H'].index[i].year for i in np.arange(0,ListofDFs['24H'].shape[0],1)]\n",
    "\n",
    "    ListofDFs['72H']['Month'] = [ListofDFs['72H'].index[i].month for i in np.arange(0,ListofDFs['72H'].shape[0],1)]\n",
    "    ListofDFs['72H']['Year'] = [ListofDFs['72H'].index[i].year for i in np.arange(0,ListofDFs['72H'].shape[0],1)]\n",
    "\n",
    "    ListofDFs['1M']['Month'] = [ListofDFs['1M'].index[i].month for i in np.arange(0,ListofDFs['1M'].shape[0],1)]\n",
    "    ListofDFs['1M']['Year'] = [ListofDFs['1M'].index[i].year for i in np.arange(0,ListofDFs['1M'].shape[0],1)]\n",
    "\n",
    "    VAR_1H = [np.nanvar(ListofDFs['1H']['value'][ListofDFs['1H']['Month']==i]) for i in range(1,13)]\n",
    "    SKEW_1H = [skew(ListofDFs['1H']['value'][ListofDFs['1H']['Month'] == i]) for i in range(1, 13)]\n",
    "    PROB_1H = [len(ListofDFs['1H']['value'][(ListofDFs['1H']['Month'] == i) & (ListofDFs['1H']['value'] < 0.2)].values)/len(ListofDFs['1H']['value'][ListofDFs['1H']['Month']==i]) for i in range(1,13)]\n",
    "    MEAN_24H = [np.nanmean(ListofDFs['24H']['value'][ListofDFs['24H']['Month']==i]) for i in range(1,13)]\n",
    "    VAR_24H = [np.nanvar(ListofDFs['24H']['value'][ListofDFs['24H']['Month']==i]) for i in range(1,13)]\n",
    "    SKEW_24H = [skew(ListofDFs['24H']['value'][ListofDFs['24H']['Month'] == i]) for i in range(1, 13)]\n",
    "    PROB_24H = [len(ListofDFs['24H']['value'][(ListofDFs['24H']['Month'] == i) & (ListofDFs['24H']['value'] < 0.2)].values)/len(ListofDFs['24H']['value'][ListofDFs['24H']['Month']==i]) for i in range(1,13)]\n",
    "\n",
    "    def getacf(MONTHNUM):\n",
    "        df=pd.DataFrame({'x': ListofDFs['24H']['value'][ListofDFs['24H']['Month']==MONTHNUM], 'x_lag': ListofDFs['24H']['value'][ListofDFs['24H']['Month']==MONTHNUM].shift(1)})\n",
    "        df.dropna(inplace=True)\n",
    "        acf,pval = scipy.stats.pearsonr(df['x'], df['x_lag'])\n",
    "        return acf\n",
    " \n",
    "    ACF_24H=[getacf(i) for i in range(1,13)]\n",
    "    VAR_72H = [np.nanvar(ListofDFs['72H']['value'][ListofDFs['72H']['Month']==i]) for i in range(1,13)]\n",
    "    VAR_1M = [np.nanvar(ListofDFs['1M']['value'][ListofDFs['1M']['Month']==i]) for i in range(1,13)]\n",
    "\n",
    "    # standardization of the statistics\n",
    "    STAN_VAR_1H = np.mean([np.nanvar(ListofDFs['1H']['value'][ListofDFs['1H']['Year']==i]) for i in np.unique(ListofDFs['1H']['Year'])])\n",
    "    STAN_SKEW_1H = np.mean([skew(ListofDFs['1H']['value'][ListofDFs['1H']['Year']==i], nan_policy='omit') for i in np.unique(ListofDFs['1H']['Year'])])\n",
    "    STAN_MEAN_24H = np.mean([np.nanmean(ListofDFs['24H']['value'][ListofDFs['24H']['Year']==i]) for i in np.arange(min(ListofDFs['24H']['Year']),max(ListofDFs['24H']['Year'])+1,1)])\n",
    "    STAN_VAR_24H = np.mean([np.nanvar(ListofDFs['24H']['value'][ListofDFs['24H']['Year']==i]) for i in np.unique(ListofDFs['24H']['Year'])])\n",
    "    STAN_SKEW_24H = np.mean([skew(ListofDFs['24H']['value'][ListofDFs['24H']['Year']==i], nan_policy='omit') for i in np.unique(ListofDFs['24H']['Year'])])\n",
    "    STAN_VAR_72H = np.mean([np.nanvar(ListofDFs['72H']['value'][ListofDFs['72H']['Year']==i]) for i in np.unique(ListofDFs['72H']['Year'])])\n",
    "    STAN_VAR_1M = np.mean([np.nanvar(ListofDFs['1M']['value'][ListofDFs['1M']['Year']==i]) for i in np.unique(ListofDFs['1M']['Year'])])\n",
    "\n",
    "    STAT =  pd.DataFrame({'statistic_id':np.repeat(range(1,11),12),\n",
    "       'name':np.repeat(statistic_definitions['name'],12),\n",
    "       'duration':np.repeat(statistic_definitions['duration'],12),\n",
    "       'month':np.tile(range(1,13),statistic_definitions.shape[0]),\n",
    "       'value':np.concatenate((VAR_1H,SKEW_1H,PROB_1H,MEAN_24H,VAR_24H,SKEW_24H,PROB_24H,ACF_24H,VAR_72H,VAR_1M)),\n",
    "       'weight':np.repeat(statistic_definitions['weight'],12),\n",
    "       'gs':np.concatenate((np.repeat(STAN_VAR_1H,12),np.repeat(STAN_SKEW_1H,12),np.repeat(1,12),np.repeat(STAN_MEAN_24H,12),np.repeat(STAN_VAR_24H,12),\n",
    "            np.repeat(STAN_SKEW_24H,12),np.repeat(1,24),np.repeat(STAN_VAR_72H,12),np.repeat(STAN_VAR_1M,12))),\n",
    "       'phi':np.repeat(1,120)})\n",
    "    \n",
    "    return STAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbc324c7-436d-46e6-a7c9-5c08d2761923",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Stage-3: Getting reference statistics stage ends here and \n",
    "##  fitting stage begins from here \n",
    "\n",
    "def prepare(statistics):\n",
    "    statistic_ids = sorted(list(set(statistics['statistic_id'])))\n",
    "\n",
    "    fitting_data = {}\n",
    "    reference_statistics = []\n",
    "    weights = []\n",
    "    gs = []\n",
    "    for statistic_id in statistic_ids:\n",
    "        df = statistics.loc[statistics['statistic_id'] == statistic_id].copy()\n",
    "\n",
    "        fitting_data[(statistic_id, 'name')] = df['name'].values[0]\n",
    "        fitting_data[(statistic_id, 'duration')] = df['duration'].values[0]\n",
    "        fitting_data[(statistic_id, 'lag')] = df['lag'].values[0]\n",
    "        fitting_data[(statistic_id, 'threshold')] = df['threshold'].values[0]\n",
    "        fitting_data[(statistic_id, 'df')] = df\n",
    "\n",
    "        reference_statistics.append(df['value'].values)\n",
    "        weights.append(df['weight'].values)\n",
    "        gs.append(df['gs'].values)\n",
    "\n",
    "    reference_statistics = np.concatenate(reference_statistics)\n",
    "    weights = np.concatenate(weights)\n",
    "    gs = np.concatenate(gs)\n",
    "\n",
    "    return statistic_ids, fitting_data, reference_statistics, weights, gs\n",
    "\n",
    "\n",
    "def fitting_wrapper_point(\n",
    "        parameters, intensity_distribution, statistic_ids, fitting_data,\n",
    "        ref_stats, weights, gs, all_parameter_names, parameters_to_fit,\n",
    "        fixed_parameters, month\n",
    "):\n",
    "    \"\"\"\n",
    "    Objective function for monthly point NSRP fitting.\n",
    "    Compares modelled statistics to observed statistics for one month.\n",
    "    \"\"\"\n",
    "    # Build full parameter dictionary\n",
    "    parameters_dict = {}\n",
    "    for pname in all_parameter_names:\n",
    "        if pname in parameters_to_fit:\n",
    "            parameters_dict[pname] = parameters[parameters_to_fit.index(pname)]\n",
    "        else:\n",
    "            parameters_dict[pname] = fixed_parameters[(month, pname)]\n",
    "\n",
    "    # Calculate model statistics\n",
    "    mod_stats = calculate_analytical_properties(\n",
    "        spatial_model=False,\n",
    "        intensity_distribution=intensity_distribution,\n",
    "        parameters_dict=parameters_dict,\n",
    "        statistic_ids=statistic_ids,\n",
    "        fitting_data=fitting_data\n",
    "    )\n",
    "\n",
    "    # Return weighted and scaled error\n",
    "    return calculate_objective_function(ref_stats, mod_stats, weights, gs)\n",
    "\n",
    "def fit_by_month_point(\n",
    "        unique_months, reference_statistics, intensity_distribution,\n",
    "        all_parameter_names, parameters_to_fit, parameter_bounds,\n",
    "        fixed_parameters, n_workers=1, stage='final', initial_parameters=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit NSRP parameters for each month independently.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    fitted_statistics = []\n",
    "\n",
    "    for month in unique_months:\n",
    "        # Filter reference statistics for current month\n",
    "        month_ref_stats = reference_statistics.loc[reference_statistics['month'] == month].copy()\n",
    "\n",
    "        # Prepare fitting data\n",
    "        statistic_ids, fitting_data, ref, weights, gs = prepare(month_ref_stats)\n",
    "\n",
    "        # Parameter bounds for this month\n",
    "        bounds = [parameter_bounds[(month, p)] for p in parameters_to_fit]\n",
    "\n",
    "        # Initial guess if available\n",
    "        x0 = initial_parameters[month] if initial_parameters is not None else None\n",
    "\n",
    "        # Run optimisation\n",
    "        result = differential_evolution(\n",
    "            func=fitting_wrapper_point,\n",
    "            bounds=bounds,\n",
    "            args=(intensity_distribution, statistic_ids, fitting_data, ref,\n",
    "                  weights, gs, all_parameter_names, parameters_to_fit,\n",
    "                  fixed_parameters, month),\n",
    "            tol=0.001,\n",
    "            updating='deferred',\n",
    "            workers=n_workers,\n",
    "            x0=x0\n",
    "        )\n",
    "\n",
    "        # Store results for this month\n",
    "        for idx, pname in enumerate(parameters_to_fit):\n",
    "            results[(pname, month)] = result.x[idx]\n",
    "        results[('converged', month)] = result.success\n",
    "        results[('objective_function', month)] = result.fun\n",
    "        results[('iterations', month)] = result.nit\n",
    "        results[('function_evaluations', month)] = result.nfev\n",
    "\n",
    "        # Build parameter dictionary for fitted stats\n",
    "        parameters_dict = {}\n",
    "        for pname in all_parameter_names:\n",
    "            if pname in parameters_to_fit:\n",
    "                parameters_dict[pname] = results[(pname, month)]\n",
    "            else:\n",
    "                parameters_dict[pname] = fixed_parameters[(month, pname)]\n",
    "\n",
    "        # Calculate fitted statistics\n",
    "        mod_stats = calculate_analytical_properties(\n",
    "            spatial_model=False,\n",
    "            intensity_distribution=intensity_distribution,\n",
    "            parameters_dict=parameters_dict,\n",
    "            statistic_ids=statistic_ids,\n",
    "            fitting_data=fitting_data\n",
    "        )\n",
    "\n",
    "        df_stats = month_ref_stats.copy()\n",
    "        df_stats['value'] = mod_stats\n",
    "        df_stats['month'] = month\n",
    "        fitted_statistics.append(df_stats)\n",
    "\n",
    "    # Format parameter table\n",
    "    parameters_df = format_results(results, all_parameter_names, parameters_to_fit, fixed_parameters, unique_months,intensity_distribution)\n",
    "    parameters_df['fit_stage'] = stage\n",
    "\n",
    "    # Combine fitted statistics\n",
    "    fitted_statistics = pd.concat(fitted_statistics)\n",
    "    fitted_statistics['fit_stage'] = stage\n",
    "\n",
    "    return parameters_df, fitted_statistics\n",
    "\n",
    "\n",
    "def calculate_analytical_properties(spatial_model, intensity_distribution, parameters_dict, statistic_ids, fitting_data):\n",
    "    \n",
    "    # Unpack parameter values common to point and spatial models\n",
    "    lamda = parameters_dict['lamda']\n",
    "    beta = parameters_dict['beta']\n",
    "    eta = parameters_dict['eta']\n",
    "    theta = parameters_dict['theta']\n",
    "\n",
    "    # Get or calculate nu\n",
    "    if not spatial_model:\n",
    "        nu = parameters_dict['nu']\n",
    "    else:\n",
    "        rho = parameters_dict['rho']\n",
    "        gamma = parameters_dict['gamma']\n",
    "        nu = 2.0 * np.pi * rho / gamma ** 2.0\n",
    "\n",
    "    # Shape parameters are only relevant to non-exponential intensity distributions\n",
    "    if intensity_distribution == 'weibull':\n",
    "        kappa = parameters_dict['kappa']\n",
    "    elif intensity_distribution == 'generalised_gamma':\n",
    "        kappa_1 = parameters_dict['kappa_1']\n",
    "        kappa_2 = parameters_dict['kappa_2']\n",
    "\n",
    "    # Calculate raw moments (1-3) of intensity distribution\n",
    "    moments = []\n",
    "    for n in [1, 2, 3]:\n",
    "        if intensity_distribution == 'exponential':\n",
    "            moments.append(scipy.stats.expon.moment(n, scale=theta))\n",
    "        elif intensity_distribution == 'weibull':\n",
    "            moments.append(scipy.stats.weibull_min.moment(n, c=kappa, scale=theta))\n",
    "        elif intensity_distribution == 'generalised_gamma':\n",
    "            moments.append(scipy.stats.gengamma.moment(n, a=(kappa_1 / kappa_2), c=kappa_2, scale=theta))\n",
    "    mu_1, mu_2, mu_3 = moments\n",
    "\n",
    "    # Duration string â†’ numeric hours mapping\n",
    "    duration_map = {\n",
    "        '1h': 1.0,\n",
    "        '24h': 24.0,\n",
    "        '72h': 72.0,\n",
    "        '1m': 24.0 * 30.0  # approx 30 days per month, adjust if needed\n",
    "    }\n",
    "\n",
    "    statistic_arrays = []\n",
    "    for statistic_id in statistic_ids:\n",
    "        name = fitting_data[(statistic_id, 'name')]\n",
    "        duration_str = fitting_data[(statistic_id, 'duration')]\n",
    "        # normalize strings to lower-case to match robustly\n",
    "        name_l = str(name).lower()\n",
    "        dur_l = str(duration_str).lower()\n",
    "\n",
    "        # convert duration to numeric hours\n",
    "        if dur_l in duration_map:\n",
    "            duration_val = duration_map[dur_l]\n",
    "        else:\n",
    "            # try to parse numeric prefix if user used formats like '1H' or '24H' etc.\n",
    "            try:\n",
    "                if dur_l.endswith('h'):\n",
    "                    duration_val = float(dur_l[:-1])\n",
    "                elif dur_l.endswith('m') and len(dur_l) > 1:  # '1M' treat as month\n",
    "                    duration_val = duration_map.get('1m', 24.0 * 30.0)\n",
    "                else:\n",
    "                    duration_val = float(dur_l)\n",
    "            except Exception:\n",
    "                raise ValueError(f\"Unknown duration string: {duration_str}\")\n",
    "\n",
    "        phi = np.ones(len(fitting_data[(statistic_id, 'df')]))\n",
    "\n",
    "        # detect statistic types using substring matching\n",
    "        is_autocorr = 'autocorrelation' in name_l\n",
    "        is_crosscorr = 'cross-correlation' in name_l or 'cross_correlation' in name_l or 'cross correlation' in name_l or 'cross' in name_l and 'correlation' in name_l\n",
    "        is_prob = 'probability_dry' in name_l or name_l.startswith('probability')\n",
    "        is_mean = 'mean' in name_l and not is_prob\n",
    "        is_variance = 'variance' in name_l\n",
    "        is_skew = 'skew' in name_l\n",
    "\n",
    "        if is_autocorr or is_crosscorr:\n",
    "            lag = fitting_data[(statistic_id, 'lag')]\n",
    "            if is_crosscorr:\n",
    "                phi2 = np.ones(len(fitting_data[(statistic_id, 'df')]))\n",
    "                distances = fitting_data[(statistic_id, 'df')].get('distance', pd.Series([])).values\n",
    "        elif is_prob:\n",
    "            threshold = fitting_data[(statistic_id, 'threshold')]\n",
    "\n",
    "        # compute appropriate statistic\n",
    "        if is_mean:\n",
    "            values = calculate_mean(duration_val, lamda, nu, mu_1, eta, phi)\n",
    "        elif is_variance:\n",
    "            values = calculate_variance(duration_val, eta, beta, lamda, nu, mu_1, mu_2, phi)\n",
    "        elif is_skew:\n",
    "            values = calculate_skewness(duration_val, eta, beta, lamda, nu, mu_1, mu_2, mu_3, phi)\n",
    "        elif is_autocorr:\n",
    "            values = calculate_autocorrelation(duration_val, lag, eta, beta, lamda, nu, mu_1, mu_2, phi)\n",
    "        elif is_prob:\n",
    "            values = calculate_probability_dry(duration_val, nu, beta, eta, lamda, phi, threshold if 'threshold' in locals() else None)\n",
    "        #elif is_crosscorr:\n",
    "            #values = calculate_cross_correlation(duration_val, lag, eta, beta, lamda, nu, mu_1, mu_2, gamma, distances, phi, phi2)\n",
    "        else:\n",
    "            # if nothing matches, raise to avoid silent mis-assignment\n",
    "            raise ValueError(f\"Unknown statistic name: {name} (normalized: {name_l})\")\n",
    "\n",
    "        statistic_arrays.append(np.atleast_1d(values))\n",
    "\n",
    "    return np.concatenate(statistic_arrays)\n",
    "\n",
    "def calculate_objective_function(ref, mod, w, sf):\n",
    "   obj_fun = np.sum((w ** 2 / sf ** 2) * (ref - mod) ** 2)\n",
    "   return obj_fun\n",
    "\n",
    "\n",
    "def _mean(h, lamda, nu, mu_X, eta, phi=1):\n",
    "    \"\"\"\n",
    "    Mean of NSRP process.Equation 2.11 in Cowpertwait (1995), which is Equation 5 in Cowpertwait et al. (2002).\n",
    "    \"\"\"\n",
    "    mean_ = phi * h * lamda * nu * mu_X / eta\n",
    "    return mean_\n",
    "\n",
    "\n",
    "def calculate_mean(duration, lamda, nu, mu_1, eta, phi):\n",
    "    mean_ = _mean(duration, lamda, nu, mu_1, eta, phi)\n",
    "    return mean_\n",
    "\n",
    "\n",
    "def _covariance_a_b_terms(h, l, eta, beta, lamda, nu, mu_X):\n",
    "    \"\"\"A and B terms needed in covariance calculations.\n",
    "\n",
    "    See Equations 2.12, 2.15 and 2.16 in Cowpertwait (1995).\n",
    "\n",
    "    \"\"\"\n",
    "    # Cowpertwait (1995) equations 2.15 and 2.16\n",
    "    if l == 0:\n",
    "        A_hl = 2 * (h * eta + np.exp(-eta * h) - 1) / eta ** 2\n",
    "        B_hl = 2 * (h * beta + np.exp(-beta * h) - 1) / beta ** 2\n",
    "    else:\n",
    "        A_hl = (1 - np.exp(-eta * h)) ** 2 * np.exp(-eta * h * (l - 1)) / eta ** 2\n",
    "        B_hl = (1 - np.exp(-beta * h)) ** 2 * np.exp(-beta * h * (l - 1)) / beta ** 2\n",
    "\n",
    "    # Cowpertwait (1995) equation 2.12\n",
    "    Aij = 0.5 * lamda * beta * nu ** 2 * mu_X ** 2 * ((2 * beta) / ((beta ** 2 - eta ** 2) * (2 * eta)))\n",
    "    Bij = -0.5 * lamda * beta * nu ** 2 * mu_X ** 2 * (1 / ((beta - eta) * (beta + eta)))\n",
    "\n",
    "    return A_hl, B_hl, Aij, Bij\n",
    "\n",
    "def _site_covariance(h, l, eta, beta, lamda, nu, mu_X, var_X, phi=1):\n",
    "    \"\"\"Covariance of NSRP process.\n",
    "\n",
    "    Covariance is calculated as Equation 2.14 in Cowpertwait (1995). This\n",
    "    requires A and B terms from calculate_A_and_B().\n",
    "\n",
    "    \"\"\"\n",
    "    A_hl, B_hl, Aij, Bij = _covariance_a_b_terms(h, l, eta, beta, lamda, nu, mu_X)\n",
    "\n",
    "    # Cowpertwait (1995) equation 2.14\n",
    "    cov = (\n",
    "            phi ** 2 * (A_hl * Aij + B_hl * Bij) + phi ** 2 * lamda * nu * var_X * A_hl / eta\n",
    "    )\n",
    "    return cov\n",
    "\n",
    "def calculate_variance(duration, eta, beta, lamda, nu, mu_1, mu_2, phi):\n",
    "    variance = _site_covariance(duration, 0, eta, beta, lamda, nu, mu_1, mu_2, phi)\n",
    "    return variance\n",
    "\n",
    "\n",
    "\n",
    "def _skewness_f(eta, beta, h):\n",
    "    \"\"\"f-function needed for calculating third central moment.\n",
    "\n",
    "    Equation 2.10 in Cowpertwait (1998), which is Equation 11 in Cowpertwait\n",
    "    et al. (2002).\n",
    "\n",
    "    \"\"\"\n",
    "    f = (\n",
    "        # line 1\n",
    "        -2 * eta ** 3 * beta ** 2 * np.exp(-eta * h) - 2 * eta ** 3 * beta ** 2 * np.exp(-beta * h)\n",
    "        + eta ** 2 * beta ** 3 * np.exp(-2 * eta * h) + 2 * eta ** 4 * beta * np.exp(-eta * h)\n",
    "        # line 2\n",
    "        + 2 * eta ** 4 * beta * np.exp(-beta * h) + 2 * eta ** 3 * beta ** 2 * np.exp(-(eta + beta) * h)\n",
    "        - 2 * eta ** 4 * beta * np.exp(-(eta + beta) * h) - 8 * eta ** 3 * beta ** 3 * h\n",
    "        # line 3\n",
    "        + 11 * eta ** 2 * beta ** 3 - 2 * eta ** 4 * beta + 2 * eta ** 3 * beta ** 2\n",
    "        + 4 * eta * beta ** 5 * h + 4 * eta ** 5 * beta * h - 7 * beta ** 5\n",
    "        # line 4\n",
    "        - 4 * eta ** 5 + 8 * beta ** 5 * np.exp(-eta * h) - beta ** 5 * np.exp(-2 * eta * h)\n",
    "        - 2 * h * eta ** 3 * beta ** 3 * np.exp(-eta * h)\n",
    "        # line 5\n",
    "        - 12 * eta ** 2 * beta ** 3 * np.exp(-eta * h) + 2 * h * eta * beta ** 5 * np.exp(-eta * h)\n",
    "        + 4 * eta ** 5 * np.exp(-beta * h)\n",
    "    )\n",
    "    return f\n",
    "\n",
    "\n",
    "def _skewness_g(eta, beta, h):\n",
    "    \"\"\"f-function needed for calculating third central moment.\n",
    "\n",
    "    Equation 2.11 in Cowpertwait (1998), which is Equation 12 in Cowpertwait\n",
    "    et al. (2002).\n",
    "\n",
    "    \"\"\"\n",
    "    g = (\n",
    "        # line 1\n",
    "        12 * eta ** 5 * beta * np.exp(-beta * h) + 9 * eta ** 4 * beta ** 2 + 12 * eta * beta ** 5 * np.exp(-eta * h)\n",
    "        + 9 * eta ** 2 * beta ** 4\n",
    "        # line 2\n",
    "        + 12 * eta ** 3 * beta ** 3 * np.exp(-(eta + beta) * h) - eta ** 2 * beta ** 4 * np.exp(-2 * eta * h)\n",
    "        - 12 * eta ** 3 * beta ** 3 * np.exp(-beta * h) - 9 * eta ** 5 * beta\n",
    "        # line 3\n",
    "        - 9 * eta * beta ** 5 - 3 * eta * beta ** 5 * np.exp(-2 * eta * h)\n",
    "        - eta ** 4 * beta ** 2 * np.exp(-2 * beta * h) - 12 * eta ** 3 * beta ** 3 * np.exp(-eta * h)\n",
    "        # line 4\n",
    "        + 6 * eta ** 5 * beta ** 2 * h - 10 * beta ** 4 * eta ** 3 * h + 6 * beta ** 5 * eta ** 2 * h\n",
    "        - 10 * beta ** 3 * eta ** 4 * h + 4 * beta ** 6 * eta * h\n",
    "        # line 5\n",
    "        - 8 * beta ** 2 * eta ** 4 * np.exp(-beta * h) + 4 * beta * eta ** 6 * h + 12 * beta ** 3 * eta ** 3\n",
    "        - 8 * beta ** 4 * eta ** 2 * np.exp(-eta * h) - 6 * eta ** 6\n",
    "        # line 6\n",
    "        - 6 * beta ** 6 - 2 * eta ** 6 * np.exp(-2 * beta * h) - 2 * beta ** 6 * np.exp(-2 * eta * h)\n",
    "        + 8 * eta ** 6 * np.exp(-beta * h)\n",
    "        # line 7\n",
    "        + 8 * beta ** 6 * np.exp(-eta * h) - 3 * beta * eta ** 5 * np.exp(-2 * beta * h)\n",
    "    )\n",
    "    return g\n",
    "\n",
    "\n",
    "def _third_central_moment(\n",
    "        h, eta, beta, lamda, nu, mu_X, var_X, X_mom3\n",
    "):\n",
    "    \"\"\"Third central moment of NSRP process.\n",
    "\n",
    "    Equation 2.9 in Cowpertwait (1998), which is Equation 10 in Cowpertwait\n",
    "    et al. (2002). Requires f-function and g-function from skewness_f() and\n",
    "    skewness_g(), respectively.\n",
    "\n",
    "    \"\"\"\n",
    "    f = _skewness_f(eta, beta, h)\n",
    "    g = _skewness_g(eta, beta, h)\n",
    "\n",
    "    # Cowpertwait (1998) equation 2.9\n",
    "    skew = (\n",
    "        # line 1\n",
    "        6 * lamda * nu * X_mom3 * (eta * h - 2 + eta * h * np.exp(-eta * h) + 2 * np.exp(-eta * h)) / eta ** 4\n",
    "        # line 2\n",
    "        + 3 * lamda * mu_X * var_X * nu ** 2 * f\n",
    "        # line 3\n",
    "        / (2 * eta ** 4 * beta * (beta ** 2 - eta ** 2) ** 2) + lamda * mu_X ** 3\n",
    "        # line 4\n",
    "        * nu ** 3 * g\n",
    "        # line 5\n",
    "        / (2 * eta ** 4 * beta * (eta ** 2 - beta ** 2) * (eta - beta) * (2 * beta + eta) * (beta + 2 * eta))\n",
    "    )\n",
    "    return skew\n",
    "\n",
    "\n",
    "\n",
    "def calculate_skewness(duration, eta, beta, lamda, nu, mu_1, mu_2, mu_3, phi):\n",
    "    unscaled_variance = _site_covariance(duration, 0, eta, beta, lamda, nu, mu_1, mu_2, phi * 0.0 + 1.0)\n",
    "    third_moment = _third_central_moment(duration, eta, beta, lamda, nu, mu_1, mu_2, mu_3)\n",
    "    skewness = third_moment / (unscaled_variance ** 0.5) ** 3\n",
    "    return skewness\n",
    "\n",
    "\n",
    "def calculate_autocorrelation(duration, lag, eta, beta, lamda, nu, mu_1, mu_2, phi):\n",
    "    variance = _site_covariance(duration, 0, eta, beta, lamda, nu, mu_1, mu_2, phi)\n",
    "    lag_covariance = _site_covariance(duration, lag, eta, beta, lamda, nu, mu_1, mu_2, phi)\n",
    "    autocorrelation = lag_covariance / variance\n",
    "    return autocorrelation\n",
    "\n",
    "\n",
    "def _omega(beta, t, eta):\n",
    "    \"\"\"Omega term in Equation 2.17 in Cowpertwait (1995).\n",
    "\n",
    "    Probability that a cell overlapping point m with arrival time in (0, t)\n",
    "    terminates before t. Same as Equation 2.15 in Cowpertwait (1994).\n",
    "\n",
    "    \"\"\"\n",
    "    omega = 1 - beta * (np.exp(-beta * t) - np.exp(-eta * t)) / ((eta - beta) * (1 - np.exp(-beta * t)))\n",
    "    return omega\n",
    "\n",
    "def _probability_zero_t_0(t, nu, beta, eta):\n",
    "    \"\"\"Probability of no rain in (0, t).\n",
    "\n",
    "    Equation 2.18 in Cowpertwait (1995) but setting t=0 and h=t.\n",
    "\n",
    "    Returns 1 minus the probability, as this is what is needed to find the dry\n",
    "    probability using Equation 2.19 in Cowpertwait (1995).\n",
    "\n",
    "    \"\"\"\n",
    "    omega_ = _omega(beta, t, eta)\n",
    "    p = np.exp(-nu + nu * np.exp(-beta * (0 + t)) + omega_ * nu * (1 - np.exp(-beta * 0)))\n",
    "    return 1 - p\n",
    "\n",
    "def _probability_zero_h_t(t, h, nu, beta, eta):\n",
    "    \"\"\"Probability of no rain in (t, t+h) due to a storm origin at time zero.\n",
    "\n",
    "    Equation 2.18 in Cowpertwait (1995). I.e. differs from Cowpertwait (1994),\n",
    "    as number of cells per storm is a Poisson random variable, whereas\n",
    "    Cowpertwait (1994) used a geometric distribution.\n",
    "\n",
    "    Returns 1 minus the probability, as this is what is needed to find the dry\n",
    "    probability using Equation 2.19 in Cowpertwait (1995).\n",
    "\n",
    "    \"\"\"\n",
    "    omega_ = _omega(beta, t, eta)\n",
    "\n",
    "    # Cowpertwait (1995) equation 2.18\n",
    "    p = np.exp(-nu + nu * np.exp(-beta * (t + h)) + omega_ * nu * (1 - np.exp(-beta * t)))\n",
    "    return 1 - p\n",
    "\n",
    "def _probability_dry(h, nu, beta, eta, lamda):\n",
    "    \"\"\"Probability dry (equal to zero) for NSRP process.\n",
    "\n",
    "    Equation 2.19 in Cowpertwait (1995).\n",
    "\n",
    "    \"\"\"\n",
    "    term1, term1_error = scipy.integrate.quad(_probability_zero_h_t, 0, np.inf, args=(h, nu, beta, eta))\n",
    "    term2, term2_error = scipy.integrate.quad(_probability_zero_t_0, 0, h, args=(nu, beta, eta))\n",
    "    p = np.exp(-lamda * term1 - lamda * term2)\n",
    "    return p\n",
    "\n",
    "\n",
    "def _probability_dry_correction(h, threshold, uncorr_pdry):\n",
    "    \"\"\"Estimation of dry probability for non-zero thresholds.\n",
    "\n",
    "    Following Section 4.3 in Burton et al. (2008). Options are only for 24hr\n",
    "    duration (thresholds of 0.2 or 1.0 mm) or 1hr duration (thresholds of 0.1 or\n",
    "    0.2 mm).\n",
    "\n",
    "    \"\"\"\n",
    "    if h == 24:\n",
    "\n",
    "        # Burton et al. (2008) equation 8\n",
    "        if threshold == 1.0:\n",
    "            if 0.15 <= uncorr_pdry <= 0.75:\n",
    "                corr_pdry = 0.05999 + 1.603 * uncorr_pdry - 0.8138 * uncorr_pdry ** 2\n",
    "            elif uncorr_pdry < 0.15:\n",
    "                dx = 0.15\n",
    "                dy = 0.2821\n",
    "                m = dy / dx\n",
    "                corr_pdry = m * uncorr_pdry\n",
    "            elif uncorr_pdry > 0.75:\n",
    "                dx = 0.75\n",
    "                dy = 0.8045\n",
    "                m = dy / dx\n",
    "                corr_pdry = m * uncorr_pdry\n",
    "\n",
    "        # Burton et al. (2008) equation 9\n",
    "        elif threshold == 0.2:\n",
    "            if 0.2 <= uncorr_pdry <= 0.75:\n",
    "                corr_pdry = 0.007402 + 1.224 * uncorr_pdry - 0.2908 * uncorr_pdry ** 2\n",
    "            elif uncorr_pdry < 0.2:\n",
    "                dx = 0.2\n",
    "                dy = 0.2405\n",
    "                m = dy / dx\n",
    "                corr_pdry = m * uncorr_pdry\n",
    "            elif uncorr_pdry > 0.75:\n",
    "                dx = 0.75\n",
    "                dy = 0.7617\n",
    "                m = dy / dx\n",
    "                corr_pdry = m * uncorr_pdry\n",
    "\n",
    "    elif h == 1:\n",
    "\n",
    "        # Burton et al. (2008) equation 10\n",
    "        if threshold == 0.1:\n",
    "            corr_pdry = 0.114703 + 0.884491 * uncorr_pdry\n",
    "\n",
    "        # Burton et al. (2008) equation 11\n",
    "        elif threshold == 0.2:\n",
    "            corr_pdry = 0.239678 + 0.758837 * uncorr_pdry\n",
    "        corr_pdry = max(corr_pdry, 0.0)\n",
    "        corr_pdry = min(corr_pdry, 1.0)\n",
    "\n",
    "    return corr_pdry\n",
    "\n",
    "\n",
    "def calculate_probability_dry(duration, nu, beta, eta, lamda, phi, threshold=0.2):\n",
    "    probability_dry = _probability_dry(duration, nu, beta, eta, lamda)\n",
    "    if threshold is not None:\n",
    "        probability_dry = _probability_dry_correction(duration, threshold, probability_dry)\n",
    "    probability_dry = phi * 0.0 + probability_dry\n",
    "    probability_dry = np.clip(probability_dry, 0.0, 1.0)\n",
    "    return probability_dry\n",
    "\n",
    "\n",
    "def format_results(results, all_parameter_names, parameters_to_fit, fixed_parameters, unique_months, intensity_distribution):\n",
    "    \"\"\"\n",
    "    Format fitted results into a consistent dataframe for NSRP point model.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Optimisation results dictionary.\n",
    "        all_parameter_names (list): All possible parameter names.\n",
    "        parameters_to_fit (list): Parameters being fitted.\n",
    "        fixed_parameters (dict): Dictionary of fixed parameters {(month,param): value}.\n",
    "        unique_months (list): List of months (1-12).\n",
    "        intensity_distribution (str): 'exponential', 'weibull', or 'generalised_gamma'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Formatted dataframe with consistent column order.\n",
    "    \"\"\"\n",
    "    dc = results.copy()\n",
    "\n",
    "    # Insert fixed parameters if missing\n",
    "    for param in all_parameter_names:\n",
    "        if param not in parameters_to_fit:\n",
    "            for m in unique_months:\n",
    "                dc[(param, m)] = fixed_parameters.get((m, param), np.nan)\n",
    "\n",
    "    # Convert dict -> dataframe\n",
    "    df = pd.DataFrame.from_dict(dc, orient='index', columns=['value'])\n",
    "    df.index = pd.MultiIndex.from_tuples(df.index, names=['field', 'month'])\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.pivot(index='month', columns='field', values='value')\n",
    "    df.sort_index(inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Cast types\n",
    "    type_map = {\n",
    "        'month': int,\n",
    "        'converged': bool,\n",
    "        'iterations': int,\n",
    "        'function_evaluations': int,\n",
    "        'objective_function': float\n",
    "    }\n",
    "    for col, t in type_map.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(t)\n",
    "\n",
    "    # Define desired order depending on distribution\n",
    "    if intensity_distribution == 'exponential':\n",
    "        desired_order = ['fit_stage','month','lamda','beta','nu','eta','theta',\n",
    "                         'converged','objective_function','iterations','function_evaluations']\n",
    "    elif intensity_distribution == 'weibull':\n",
    "        desired_order = ['fit_stage','month','lamda','beta','nu','eta','theta','kappa',\n",
    "                         'converged','objective_function','iterations','function_evaluations']\n",
    "    elif intensity_distribution == 'generalised_gamma':\n",
    "        desired_order = ['fit_stage','month','lamda','beta','nu','eta','theta','kappa_1','kappa_2',\n",
    "                         'converged','objective_function','iterations','function_evaluations']\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown distribution: {intensity_distribution}\")\n",
    "\n",
    "    # Reorder, keeping only existing columns\n",
    "    df = df.reindex(columns=[c for c in desired_order if c in df.columns])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c304f71-6201-4189-a766-b982d7ad9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage-4: Fitting stage ends here and NSRP simulation stage begins ##\n",
    "#############################################\n",
    "# The steps in simulation of the NSRP process are:  \n",
    "#     1. Simulate storms as a temporal Poisson process.\n",
    "#     2. Simulate raincells.\n",
    "#     3. Simulate raincell arrival times.\n",
    "#     4. Simulate raincell durations.\n",
    "#     5. Simulate raincell intensities.\n",
    "\n",
    "def simulate_storms(month_lengths, simulation_length, parameters, rng):\n",
    "    simulation_end_time = np.cumsum(month_lengths)[-1]\n",
    "\n",
    "    sim_len_ext = simulation_length + 4\n",
    "    month_lengths_ext = month_lengths.copy()\n",
    "    for _ in range(4):\n",
    "        month_lengths_ext = np.concatenate([month_lengths_ext, month_lengths[-12:]])\n",
    "\n",
    "    lamda = np.tile(parameters['lamda'].values, sim_len_ext)\n",
    "    cumulative_expected_storms = np.cumsum(lamda * month_lengths_ext)\n",
    "    cumulative_month_endtimes = np.cumsum(month_lengths_ext)\n",
    "    expected_number_of_storms = cumulative_expected_storms[-1]\n",
    "    number_of_storms = rng.poisson(expected_number_of_storms)\n",
    "\n",
    "    deformed_arrivals = expected_number_of_storms * np.sort(rng.uniform(size=number_of_storms))\n",
    "    cumulative_expected_storms = np.insert(cumulative_expected_storms, 0, 0.0)\n",
    "    cumulative_month_endtimes = np.insert(cumulative_month_endtimes, 0, 0.0)\n",
    "    interpolator = scipy.interpolate.interp1d(\n",
    "        cumulative_expected_storms, cumulative_month_endtimes\n",
    "    )\n",
    "    storm_arrival_times = interpolator(deformed_arrivals)\n",
    "\n",
    "    storm_arrival_times = storm_arrival_times[storm_arrival_times < simulation_end_time]\n",
    "    number_of_storms = storm_arrival_times.shape[0]\n",
    "    storms = pd.DataFrame({\n",
    "        'storm_id': np.arange(number_of_storms),\n",
    "        'storm_arrival': storm_arrival_times\n",
    "    })\n",
    "    storms['month'] = lookup_months(month_lengths, simulation_length, storms['storm_arrival'].values)\n",
    "    return storms, number_of_storms\n",
    "\n",
    "\n",
    "def lookup_months(month_lengths, period_length, times):\n",
    "    end_times = np.cumsum(month_lengths)\n",
    "    repeated_months = np.tile(np.arange(1, 13, dtype=int), period_length)\n",
    "    idx = np.digitize(times, end_times)\n",
    "    return repeated_months[idx]\n",
    "\n",
    "\n",
    "def simulate_raincells_point(storms, parameters, rng):\n",
    "    tmp = pd.merge(storms, parameters, how='left', on='month')\n",
    "    tmp.sort_values(['storm_id'], inplace=True)\n",
    "\n",
    "    number_of_raincells = rng.poisson(tmp['nu'].values)\n",
    "    storm_ids, storm_arrivals, storm_months = make_storm_arrays_by_raincell(\n",
    "        number_of_raincells,\n",
    "        storms['storm_id'].values,\n",
    "        storms['storm_arrival'].values,\n",
    "        storms['month'].values\n",
    "    )\n",
    "    return pd.DataFrame({\n",
    "        'storm_id': storm_ids,\n",
    "        'storm_arrival': storm_arrivals,\n",
    "        'month': storm_months\n",
    "    })\n",
    "\n",
    "\n",
    "def make_storm_arrays_by_raincell(num_cells, storm_ids, storm_arrivals, storm_months):\n",
    "    return (\n",
    "        np.repeat(storm_ids, num_cells),\n",
    "        np.repeat(storm_arrivals, num_cells),\n",
    "        np.repeat(storm_months, num_cells)\n",
    "    )\n",
    "\n",
    "\n",
    "def merge_parameters(df, month_lengths, simulation_length, parameters):\n",
    "    df['month'] = lookup_months(month_lengths, simulation_length, df['storm_arrival'].values)\n",
    "    parameters_subset = parameters.drop(\n",
    "        ['fit_stage', 'converged', 'objective_function',\n",
    "         'iterations', 'function_evaluations'],\n",
    "        axis=1, errors='ignore'\n",
    "    )\n",
    "    return pd.merge(df, parameters_subset, how='left', on='month')\n",
    "\n",
    "\n",
    "def main_point_model_monthly(parameters,simulation_length,month_lengths,intensity_distribution,rng):\n",
    "    \"\"\"\n",
    "    NSRP point model simulation (monthly parameters).\n",
    "\n",
    "    Args:\n",
    "        parameters (pandas.DataFrame): Parameters dataframe from fitting (must include 'month', 'lamda', 'beta', 'nu', 'eta', 'theta', etc.).\n",
    "        simulation_length (int): Number of years to simulate.\n",
    "        month_lengths (numpy.ndarray): Hours in each month to be simulated.\n",
    "        intensity_distribution (str): Raincell intensity distribution ('exponential', 'weibull', 'generalised_gamma').\n",
    "        rng (numpy.random.Generator): Random number generator.\n",
    "\n",
    "    Steps:\n",
    "        1. Simulate storms (temporal Poisson process).\n",
    "        2. Simulate raincells for each storm.\n",
    "        3. Simulate raincell arrival times.\n",
    "        4. Simulate raincell durations.\n",
    "        5. Simulate raincell intensities.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure dataframe is sorted by month\n",
    "    parameters = parameters.copy()\n",
    "    parameters.sort_values(by='month', inplace=True)\n",
    "\n",
    "    # Step 1 - Simulate storms\n",
    "    storms, number_of_storms = simulate_storms(month_lengths, simulation_length, parameters, rng)\n",
    "\n",
    "    # Step 2 - Simulate raincells\n",
    "    df = simulate_raincells_point(storms, parameters, rng)\n",
    "\n",
    "    # Merge parameters into master dataframe\n",
    "    df = pd.merge(df, parameters, how='left', on='month')\n",
    "\n",
    "    # Step 3 - Raincell arrival times\n",
    "    raincell_arrival_times = rng.exponential(1.0 / df['beta'])  # relative to storm origin\n",
    "    df['raincell_arrival'] = df['storm_arrival'] + raincell_arrival_times\n",
    "\n",
    "    # Step 4 - Raincell durations\n",
    "    df['raincell_duration'] = rng.exponential(1.0 / df['eta'])\n",
    "    df['raincell_end'] = df['raincell_arrival'] + df['raincell_duration']\n",
    "\n",
    "    # Step 5 - Raincell intensities\n",
    "    if intensity_distribution == 'exponential':\n",
    "        df['raincell_intensity'] = rng.exponential(df['theta'])\n",
    "    elif intensity_distribution == 'weibull':\n",
    "        df['raincell_intensity'] = scipy.stats.weibull_min.rvs(\n",
    "            c=df['kappa'], scale=df['theta'], random_state=rng\n",
    "        )\n",
    "    elif intensity_distribution == 'generalised_gamma':\n",
    "        df['raincell_intensity'] = scipy.stats.gengamma.rvs(\n",
    "            a=(df['kappa_1'] / df['kappa_2']), c=df['kappa_2'],\n",
    "            scale=df['theta'], random_state=rng\n",
    "        )\n",
    "\n",
    "    # Clean up parameters from output (optional)\n",
    "    df.drop(columns=['lamda', 'beta', 'rho', 'eta', 'gamma', 'theta', 'kappa'],\n",
    "            inplace=True, errors='ignore')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def initialise_discrete_rainfall_arrays_point(n_timesteps):\n",
    "    \"\"\"\n",
    "    Create zero-filled array for point rainfall output.\n",
    "\n",
    "    Args:\n",
    "        n_timesteps (int): Total number of timesteps in the simulation.\n",
    "\n",
    "    Returns:\n",
    "        dict: {'point': np.ndarray of shape (n_timesteps, 1)}\n",
    "    \"\"\"\n",
    "    return {'point': np.zeros((n_timesteps, 1))}\n",
    "\n",
    "\n",
    "def discretise_point(period_start_time, timestep_length,\n",
    "                     raincell_arrival_times, raincell_end_times,\n",
    "                     raincell_intensities, discrete_rainfall):\n",
    "    \"\"\"\n",
    "    Convert raincells into discrete timestep rainfall totals.\n",
    "\n",
    "    Args:\n",
    "        period_start_time (float): Simulation start time in hours.\n",
    "        timestep_length (float): Length of each timestep in hours.\n",
    "        raincell_arrival_times (np.ndarray): Raincell start times in hours.\n",
    "        raincell_end_times (np.ndarray): Raincell end times in hours.\n",
    "        raincell_intensities (np.ndarray): Raincell intensities (mm/hr).\n",
    "        discrete_rainfall (np.ndarray): Array to store output (modified in place).\n",
    "    \"\"\"\n",
    "    discrete_rainfall.fill(0.0)  # Reset to zero before filling\n",
    "\n",
    "    for idx in range(raincell_arrival_times.shape[0]):\n",
    "        # Times relative to simulation/block start\n",
    "        rc_arrival_time = raincell_arrival_times[idx] - period_start_time\n",
    "        rc_end_time = raincell_end_times[idx] - period_start_time\n",
    "        rc_intensity = raincell_intensities[idx]\n",
    "\n",
    "        # Timesteps covered\n",
    "        rc_arrival_timestep = int(np.floor(rc_arrival_time / timestep_length))\n",
    "        rc_end_timestep = int(np.floor(rc_end_time / timestep_length))\n",
    "\n",
    "        # Distribute intensity across affected timesteps\n",
    "        for timestep in range(rc_arrival_timestep, rc_end_timestep + 1):\n",
    "            timestep_start_time = timestep * timestep_length\n",
    "            timestep_end_time = (timestep + 1) * timestep_length\n",
    "            effective_start = max(rc_arrival_time, timestep_start_time)\n",
    "            effective_end = min(rc_end_time, timestep_end_time)\n",
    "            timestep_coverage = effective_end - effective_start\n",
    "\n",
    "            if timestep < discrete_rainfall.shape[0] and timestep_coverage > 0:\n",
    "                discrete_rainfall[timestep, 0] += rc_intensity * timestep_coverage\n",
    "\n",
    "\n",
    "def get_storm_depths_point(df):\n",
    "    \"\"\"\n",
    "    Summarise storm total depth and duration from raincell dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): NSRP raincell output with columns:\n",
    "            ['storm_id', 'storm_arrival', 'month', 'raincell_duration', 'raincell_intensity', 'raincell_end']\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Storm-level statistics.\n",
    "    \"\"\"\n",
    "    df['raincell_depth'] = df['raincell_duration'] * df['raincell_intensity']\n",
    "\n",
    "    storm_stats = df.groupby(['storm_id']).agg({\n",
    "        'storm_arrival': 'min',\n",
    "        'month': 'min',\n",
    "        'raincell_depth': 'sum',\n",
    "        'raincell_end': 'max'\n",
    "    }).reset_index()\n",
    "\n",
    "    storm_stats.rename(columns={'raincell_depth': 'storm_depth',\n",
    "                                'raincell_end': 'storm_end'}, inplace=True)\n",
    "    storm_stats['storm_duration'] = storm_stats['storm_end'] - storm_stats['storm_arrival']\n",
    "    storm_stats.drop(columns=['storm_end'], inplace=True)\n",
    "\n",
    "    return storm_stats\n",
    "\n",
    "\n",
    "def discretise_by_point_monthly(df_raincells, simulation_length_years,InputTimeSeries, timestep_length=1.0):\n",
    "    \"\"\"\n",
    "    Discretise NSRP raincells into a continuous hourly rainfall series\n",
    "    for the full simulation period.\n",
    "\n",
    "    Args:\n",
    "        df_raincells (pd.DataFrame): Output from main_point_model_monthly().\n",
    "        simulation_length_years (int): Number of years simulated.\n",
    "        timestep_length (float): Length of timestep in hours (default=1 hour).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of hourly rainfall depths for the full simulation period.\n",
    "    \"\"\"\n",
    "    Start = pd.to_datetime(str(InputTimeSeries.index.year[0])+str('-01-01 00:00:00'),format='%Y-%m-%d %H:%M:%S')\n",
    "    End = pd.to_datetime(str(InputTimeSeries.index.year[0]+simulation_length_years-1)+str('-12-31 23:00:00'),format='%Y-%m-%d %H:%M:%S')\n",
    "    DatesVect = pd.date_range(start=Start,end=End,freq=\"H\")\n",
    "    monthly_df = pd.DataFrame({'Year':DatesVect.year,'Month':DatesVect.month})\n",
    "    HRCOUNT = monthly_df.groupby(['Year', 'Month']).size().reset_index(name='n_hours')\n",
    "    month_lengths_array = HRCOUNT['n_hours'].to_numpy()\n",
    "    \n",
    "    # Initialise the discrete rainfall array (hourly resolution)\n",
    "    total_hours = month_lengths_array.sum()\n",
    "    discrete_rainfall = np.zeros((total_hours, 1))\n",
    "\n",
    "    # Loop over months and discretise each month\n",
    "    start_hour = 0\n",
    "    for month_idx, hours_in_month in enumerate(month_lengths_array):\n",
    "        end_hour = start_hour + hours_in_month\n",
    "\n",
    "        # Get raincells that overlap with this month\n",
    "        mask = (\n",
    "            (df_raincells['raincell_arrival'] < end_hour) &\n",
    "            (df_raincells['raincell_end'] > start_hour)\n",
    "        )\n",
    "        raincells_month = df_raincells.loc[mask]\n",
    "\n",
    "        # Discretise rainfall for this month\n",
    "        discretise_point(\n",
    "            period_start_time=start_hour,\n",
    "            timestep_length=timestep_length,\n",
    "            raincell_arrival_times=raincells_month['raincell_arrival'].values,\n",
    "            raincell_end_times=raincells_month['raincell_end'].values,\n",
    "            raincell_intensities=raincells_month['raincell_intensity'].values,\n",
    "            discrete_rainfall=discrete_rainfall[start_hour:end_hour]\n",
    "        )\n",
    "\n",
    "        start_hour = end_hour\n",
    "\n",
    "    return discrete_rainfall\n",
    "\n",
    "def initialise_hourly_array(total_hours):\n",
    "    # One column for point model, all zeros initially\n",
    "    return np.zeros((total_hours, 1), dtype=float)\n",
    "\n",
    "##This marks the end of functions relevant to NSRP simulation stage. Now we put these functions to use and the Stage-5 begins ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee318eb7-15c1-4797-8d71-fb66944c3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb = pd.read_csv('Shawbury.csv')\n",
    "dfb.columns=['datetime','value']\n",
    "dfb['datetime'] = pd.to_datetime(dfb['datetime'],format='%Y-%m-%d %H:%M:%S') \n",
    "dfb.index = dfb['datetime']\n",
    "SD = {12: 1, 1: 1, 2: 1,  \n",
    "     3: 2, 4: 2, 5: 2,    \n",
    "     6: 3, 7: 3, 8: 3,    \n",
    "     9: 4,10: 4,11: 4}    \n",
    "\n",
    "ALLDF = prepare_point_timeseries(dfb,season_definitions=SD,completeness_threshold=0,durations=['1H','24H','72H','1M'] ,outlier_method='trim',maximum_relative_difference=2,maximum_alterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af40f142-0b40-4df4-a724-e1feba968c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_statistics = GetMonthStats(ALLDF)\n",
    "reference_statistics.loc[reference_statistics['name'] == 'autocorrelation_lag1','lag'] = 1\n",
    "reference_statistics.loc[reference_statistics['name'] == 'probability_dry_0.2mm','threshold'] = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51064918-a739-4526-b8e2-4a6ccd8fdd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_months = list(range(1,13))\n",
    "all_parameter_names = ['lamda', 'beta', 'eta', 'nu', 'theta','kappa']\n",
    "parameters_to_fit = ['lamda', 'beta', 'eta', 'nu', 'theta','kappa']\n",
    "fixed_parameters = {}\n",
    "parameter_bounds = {\n",
    "    (m, param): bounds\n",
    "    for m in range(1, 13)\n",
    "    for param, bounds in {\n",
    "        'lamda': (0.00001, 0.02),\n",
    "        'beta': (0.02, 1),\n",
    "        'eta': (0.1, 60),\n",
    "        'nu': (0.1, 30),\n",
    "        'theta': (0.25, 100),\n",
    "        'kappa': (0.5,1)\n",
    "    }.items()\n",
    "}\n",
    "\n",
    "def fit_month_task(month):\n",
    "    return fit_by_month_point(unique_months = [month], reference_statistics = reference_statistics,\n",
    "                              intensity_distribution =  'weibull',all_parameter_names = all_parameter_names, \n",
    "                              parameters_to_fit = parameters_to_fit, parameter_bounds = parameter_bounds, \n",
    "                              fixed_parameters = fixed_parameters)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    months = unique_months \n",
    "    with Pool() as pool:   \n",
    "        results = pool.map(fit_month_task, months)\n",
    "    parameters_df = pd.concat([res[0] for res in results], axis=0)\n",
    "    fitted_stats = pd.concat([res[1] for res in results], axis=0)\n",
    "\n",
    "parameters_df.columns = ['month','lamda','beta','nu','eta','theta','kappa','converged','objective_function','iterations','function_evaluations','fit_stage']\n",
    "\n",
    "numeric_cols = ['lamda','beta','nu','eta','theta','kappa']\n",
    "for col in numeric_cols:\n",
    "    if col in parameters_df.columns:\n",
    "        parameters_df[col] = pd.to_numeric(parameters_df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c144b-0ff1-4e78-bccb-3921efde5e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_realizations = 300\n",
    "n_years = 80\n",
    "base_seed = 42\n",
    "rng = np.random.default_rng(seed = base_seed)\n",
    "\n",
    "def GetMonthLengths2(NUM):\n",
    "    n_years = NUM\n",
    "    Start = pd.to_datetime(str(dfb.index.year[0])+str('-01-01 00:00:00'),format='%Y-%m-%d %H:%M:%S')\n",
    "    End = pd.to_datetime(str(dfb.index.year[0]+n_years-1)+str('-12-31 23:00:00'),format='%Y-%m-%d %H:%M:%S')\n",
    "    DatesVect = pd.date_range(start=Start,end=End,freq=\"H\")\n",
    "    monthly_df = pd.DataFrame({'Year':DatesVect.year,'Month':DatesVect.month})\n",
    "    HRCOUNT = monthly_df.groupby(['Year', 'Month']).size().reset_index(name='n_hours')\n",
    "    return DatesVect,HRCOUNT['n_hours'].to_numpy()\n",
    "\n",
    "\n",
    "month_lengths_array = GetMonthLengths2(n_years)[1]\n",
    "realizations = []\n",
    "\n",
    "for i in range(n_realizations):\n",
    "    rng = np.random.default_rng(seed = base_seed + i)  \n",
    "    sim_df = main_point_model_monthly(parameters=parameters_df, simulation_length=n_years, month_lengths=month_lengths_array, intensity_distribution='weibull',rng=rng)\n",
    "    HR = discretise_by_point_monthly(sim_df, simulation_length_years=n_years,InputTimeSeries = dfb)\n",
    "    HR_df = pd.DataFrame(HR, columns=['rainfall'])\n",
    "    realizations.append(HR_df)\n",
    "\n",
    "\n",
    "all_realizations_df = pd.concat(realizations, ignore_index=True,axis=1)\n",
    "all_realizations_df.columns = ['Realization'+'_'+str(i+1) for i in range(n_realizations)]\n",
    "all_realizations_df['DateTime'] = GetMonthLengths2(n_years)[0]\n",
    "all_realizations_df['Year'] = [all_realizations_df['DateTime'][i].year for i in range(all_realizations_df.shape[0])]\n",
    "all_realizations_df['Month'] = [all_realizations_df['DateTime'][i].month for i in range(all_realizations_df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "697199e1-6ea9-4174-b16a-899d8c32e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store outputs #\n",
    "# os.chdir('/home/users/azhar199/DATA/Single_Site_Outputs')\n",
    "# all_realizations_df.to_csv('NSRP_WEIB_WestFreugh_AZ.csv',index = False)\n",
    "# fitted_stats.to_csv('NSRP_WEIB_WestFreugh_FittedStatistics.csv',index = False)\n",
    "# parameters_df.to_csv('NSRP_WEIB_WestFreugh_Parameters.csv',index = False)\n",
    "# reference_statistics.to_csv('NSRP_WEIB_WestFreugh_RefStat.csv',index = False)\n",
    "# ALLDF['1H'].drop('season',axis=1).to_csv('WestFreugh_1H.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fb66c82-f922-48b5-a116-3c3b9b76ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosis of the results start from here #\n",
    "# Firstly the statistics of observed time series of rainfall are computed #\n",
    "# These statistics include - annual rainfall depths, mean monthly rainfall depths, monthly skewness, variance and dry probabilities and ACF-lag1 #\n",
    "\n",
    "obs_annual_sum = (ALLDF['1H'].groupby(['Year'])['value'].sum().reset_index())\n",
    "\n",
    "monthly_sum_df = (ALLDF['1H'].groupby(['Year', 'Month'])['value'].sum().reset_index())\n",
    "obs_mon_mean = monthly_sum_df.groupby('Month')['value'].mean()\n",
    "\n",
    "monthly_var_df = (ALLDF['1H'].groupby(['Year', 'Month'])['value'].var().reset_index())\n",
    "obs_mon_var = monthly_var_df.groupby('Month')['value'].mean()\n",
    "\n",
    "monthly_skew_df = (ALLDF['1H'].groupby(['Year', 'Month'])['value'].skew().reset_index())\n",
    "obs_mon_skew = monthly_skew_df.groupby('Month')['value'].mean()\n",
    "\n",
    "obs_mon_acf = reference_statistics['value'][(reference_statistics['duration']=='24H') & (reference_statistics['name'] == 'autocorrelation_lag1')]\n",
    "obs_mon_pdry = reference_statistics['value'][(reference_statistics['duration']=='1H') & (reference_statistics['name'] == 'probability_dry_0.2mm')]\n",
    "\n",
    "obs_monthly_stats = pd.DataFrame({\n",
    "    'Month': range(1, 13),\n",
    "    'Mean': obs_mon_mean.values,\n",
    "    'Variance': obs_mon_var.values,\n",
    "    'Skewness': obs_mon_skew.values,\n",
    "    'ACF_lag1': obs_mon_acf.values,\n",
    "    'Pdry_0.2mm': obs_mon_pdry\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89dd62c5-6727-46ab-b026-8e9455654569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_554/3906795002.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dry_mask['Month'] = SUBSET['Month']\n"
     ]
    }
   ],
   "source": [
    "# Statistics mentioned in the previous cell are now computed for all the realizations #\n",
    "\n",
    "SUBSET = all_realizations_df[all_realizations_df['Year'].isin(np.arange(ALLDF['1H']['Year'].min(),ALLDF['1H']['Year'].max()+1))]\n",
    "REALIZATION_ANN_SUM = SUBSET.groupby('Year')[['Realization_' + str(j+1) for j in range(n_realizations)]].sum().reset_index()\n",
    "\n",
    "real_cols = [f\"Realization_{i+1}\" for i in range(n_realizations)]\n",
    "monthly_sum_real = SUBSET.groupby(['Year', 'Month'])[real_cols].sum().reset_index()\n",
    "monthly_mean_real = monthly_sum_real.groupby('Month')[real_cols].mean().reset_index()\n",
    "\n",
    "monthly_var_real = SUBSET.groupby(['Year', 'Month'])[real_cols].var().reset_index()\n",
    "monthly_meanvar_real = monthly_var_real.groupby('Month')[real_cols].mean().reset_index()\n",
    "\n",
    "monthly_skew_real = SUBSET.groupby(['Year', 'Month'])[real_cols].skew().reset_index()\n",
    "monthly_meanskew_real = monthly_skew_real.groupby('Month')[real_cols].mean().reset_index()\n",
    "\n",
    "dry_mask = (SUBSET[real_cols] < 0.2).astype(int)\n",
    "dry_mask['Month'] = SUBSET['Month']\n",
    "monthly_pdry_real = dry_mask.groupby('Month')[real_cols].mean().reset_index()\n",
    "\n",
    "COL = SUBSET.columns[SUBSET.columns.str.contains('Realization_')]\n",
    "daily_df_real =[]\n",
    "for i in COL:\n",
    "    daily_sum = (SUBSET.groupby(pd.Grouper(key='DateTime', freq='D'))[i].sum().reset_index())\n",
    "    daily_df_real.append(daily_sum[daily_sum.columns[1]])\n",
    "    \n",
    "DAILY_DF_REAL=pd.concat(daily_df_real,axis=1)\n",
    "DAILY_DF_REAL['Date'] = daily_sum[daily_sum.columns[0]]\n",
    "DAILY_DF_REAL['Month'] = [DAILY_DF_REAL['Date'][i].month for i in range(0,DAILY_DF_REAL.shape[0])]\n",
    "\n",
    "def getacf(DFNUM, MONTHNUM):\n",
    "    df = pd.DataFrame({'x': DAILY_DF_REAL[DFNUM][DAILY_DF_REAL['Month'] == MONTHNUM],'x_lag': DAILY_DF_REAL[DFNUM][DAILY_DF_REAL['Month'] == MONTHNUM].shift(1)})\n",
    "    df.dropna(inplace=True)\n",
    "    if len(df) > 1:\n",
    "       acf, pval = scipy.stats.pearsonr(df['x'], df['x_lag'])\n",
    "    else:\n",
    "        acf = np.nan\n",
    "    return acf\n",
    "    \n",
    "monthly_acf_real = pd.DataFrame([[getacf(h, j) for h in COL] for j in range(1, 13)],index=range(1, 13),columns=COL)\n",
    "monthly_acf_real['Month'] = range(1,13)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41aeae4-ad87-4b84-afff-aadd44364e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_cols = [c for c in REALIZATION_ANN_SUM.columns if c.startswith('Realization_')]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()  # flatten 2x3 grid to 1D array\n",
    "\n",
    "# List of month names\n",
    "month_names = [calendar.month_abbr[i] for i in range(1, 13)]\n",
    "realization_handle = Line2D([0], [0], color='darkgrey', linewidth=0.8, alpha=0.6, label='Realizations')\n",
    "\n",
    "#  Annual plot\n",
    "axes[0].plot(REALIZATION_ANN_SUM['Year'], REALIZATION_ANN_SUM[real_cols], color='darkgrey', linewidth=0.8, alpha=0.6)\n",
    "axes[0].plot(obs_annual_sum['Year'], obs_annual_sum['value'], marker='o', color='red', linewidth=2, label='Observed')\n",
    "axes[0].set_title('Annual Sum', fontweight='bold')\n",
    "axes[0].set_xlabel('Year', fontweight='bold')\n",
    "axes[0].set_ylabel('Rainfall', fontweight='bold')\n",
    "axes[0].legend(handles=[realization_handle, axes[0].lines[-1]])\n",
    "#axes[0].set_xticklabels(obs_annual_sum['Year'], fontweight='bold')\n",
    "axes[0].set_yticklabels(axes[0].get_yticks(), fontweight='bold')\n",
    "\n",
    "#  Monthly Mean\n",
    "axes[1].plot(monthly_mean_real['Month'], monthly_mean_real[real_cols], color='darkgrey', linewidth=0.8, alpha=0.6)\n",
    "axes[1].plot(obs_monthly_stats['Month'], obs_monthly_stats['Mean'], marker='o', color='red', linewidth=2)\n",
    "axes[1].set_title('Monthly Mean Rainfall Depth', fontweight='bold')\n",
    "axes[1].set_xlabel('Month', fontweight='bold')\n",
    "axes[1].set_ylabel('Rainfall', fontweight='bold')\n",
    "axes[1].set_xticks(range(1, 13))\n",
    "axes[1].set_xticklabels(month_names, fontweight='bold')\n",
    "axes[1].set_yticklabels(axes[1].get_yticks(), fontweight='bold')\n",
    "\n",
    "#  Monthly Variance\n",
    "axes[2].plot(monthly_meanvar_real['Month'], monthly_meanvar_real[real_cols], color='darkgrey', linewidth=0.8, alpha=0.6)\n",
    "axes[2].plot(obs_monthly_stats['Month'], obs_monthly_stats['Variance'], marker='o', color='red', linewidth=2)\n",
    "axes[2].set_title('Daily Variance', fontweight='bold')\n",
    "axes[2].set_xlabel('Month', fontweight='bold')\n",
    "axes[2].set_ylabel('Variance', fontweight='bold')\n",
    "axes[2].set_xticks(range(1, 13))\n",
    "axes[2].set_xticklabels(month_names, fontweight='bold')\n",
    "axes[2].set_yticklabels(np.round(axes[2].get_yticks(),2), fontweight='bold')\n",
    "\n",
    "#  Monthly Skewness\n",
    "axes[3].plot(monthly_meanskew_real['Month'], monthly_meanskew_real[real_cols], color='darkgrey', linewidth=0.8, alpha=0.6)\n",
    "axes[3].plot(obs_monthly_stats['Month'], obs_monthly_stats['Skewness'], marker='o', color='red', linewidth=2)\n",
    "axes[3].set_title('Daily Skewness', fontweight='bold')\n",
    "axes[3].set_xlabel('Month', fontweight='bold')\n",
    "axes[3].set_ylabel('Skewness', fontweight='bold')\n",
    "axes[3].set_xticks(range(1, 13))\n",
    "axes[3].set_xticklabels(month_names, fontweight='bold')\n",
    "axes[3].set_yticklabels(axes[3].get_yticks(), fontweight='bold')\n",
    "\n",
    "#  Monthly ACF lag-1\n",
    "axes[4].plot(monthly_acf_real['Month'], monthly_acf_real[real_cols], color='darkgrey', linewidth=0.8, alpha=0.6)\n",
    "axes[4].plot(obs_monthly_stats['Month'], obs_monthly_stats['ACF_lag1'], marker='o', color='red', linewidth=2)\n",
    "axes[4].set_title('Lag-1 Autocorrelation', fontweight='bold')\n",
    "axes[4].set_xlabel('Month', fontweight='bold')\n",
    "axes[4].set_ylabel('ACF', fontweight='bold')\n",
    "axes[4].set_xticks(range(1, 13))\n",
    "axes[4].set_xticklabels(month_names, fontweight='bold')\n",
    "axes[4].set_yticklabels(np.round(axes[4].get_yticks(),2), fontweight='bold')\n",
    "\n",
    "#  Monthly Pdry 0.2 mm\n",
    "axes[5].plot(monthly_pdry_real['Month'], monthly_pdry_real[real_cols], color='darkgrey', linewidth=0.8, alpha=0.6)\n",
    "axes[5].plot(obs_monthly_stats['Month'], obs_monthly_stats['Pdry_0.2mm'], marker='o', color='red', linewidth=2)\n",
    "axes[5].set_title('Dry Probability (0.2 mm)', fontweight='bold')\n",
    "axes[5].set_xlabel('Month', fontweight='bold')\n",
    "axes[5].set_ylabel('Probability', fontweight='bold')\n",
    "axes[5].set_xticks(range(1, 13))\n",
    "axes[5].set_xticklabels(month_names, fontweight='bold')\n",
    "axes[5].set_yticklabels(np.round(axes[5].get_yticks(),2), fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "#os.chdir('/home/users/azhar199/DATA/Single_Site_Outputs')\n",
    "#plt.savefig(\"Stats.png\", dpi=600, bbox_inches='tight')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1124c915-3116-40a7-9e04-16bee3eee50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_554/3444822967.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  REAL_RP_DF['RP'] = np.arange(5,55,5)\n"
     ]
    }
   ],
   "source": [
    "# Checking the rainfall depths for various return periods #\n",
    "\n",
    "def main(rvs):\n",
    "    shape, loc, scale = gev.fit(rvs)\n",
    "    return shape, loc, scale\n",
    "\n",
    "OBS_MAX = ALLDF['1H'].groupby('Year')['value'].agg(max)\n",
    "GEV_PAR = main(OBS_MAX)\n",
    "\n",
    "def gev_return_level(mu, sigma, xi, N):\n",
    "    term = (-np.log(1 - 1.0/N)) ** (-xi)\n",
    "    RL = mu - (sigma/xi) * (1 - term)\n",
    "    return RL\n",
    "\n",
    "RPVAL = [gev_return_level(mu = GEV_PAR[1], sigma = GEV_PAR[2], xi = GEV_PAR[0], N = i) for i in np.arange(5,55,5)]\n",
    "RPVAL_DF_OBS = pd.DataFrame({'RP':np.arange(5,55,5),'RF':RPVAL})\n",
    "\n",
    "all_realizations_df2 = all_realizations_df[all_realizations_df['Year'].isin( np.arange(np.min(ALLDF['1H']['Year']), np.max(ALLDF['1H']['Year'])+1,1) )]\n",
    "\n",
    "REAL_MAX = []\n",
    "for i in real_cols:\n",
    "    real_max = all_realizations_df2.groupby('Year')[i].agg(max)\n",
    "    REAL_MAX.append(real_max)\n",
    "\n",
    "GEV_PAR_REAL = [main(REAL_MAX[i]) for i in np.arange(0,len(REAL_MAX),1)]\n",
    "\n",
    "RPVAL_DF_REAL = []\n",
    "for b in np.arange(0,len(GEV_PAR_REAL),1):\n",
    "    rpval = [gev_return_level(mu = GEV_PAR_REAL[b][1], sigma = GEV_PAR_REAL[b][2], xi = GEV_PAR_REAL[b][0], N = i) for i in np.arange(5,55,5)]\n",
    "    rpval_df = pd.DataFrame({'RF':rpval})\n",
    "    RPVAL_DF_REAL.append(rpval_df)\n",
    "\n",
    "REAL_RP_DF = pd.concat(RPVAL_DF_REAL,axis=1)\n",
    "REAL_RP_DF.columns = real_cols \n",
    "REAL_RP_DF['RP'] = np.arange(5,55,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29660d-5bc0-409c-9463-f15603c95207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = REAL_RP_DF.melt(id_vars=\"RP\", value_vars=real_cols, var_name=\"Realization\", value_name=\"Value\")\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.rcParams.update({'font.weight': 'bold'})\n",
    "\n",
    "sns.boxplot(\n",
    "    data=df_long, \n",
    "    x=\"RP\", \n",
    "    y=\"Value\",\n",
    "    boxprops=dict(facecolor='none', edgecolor='blue'),  \n",
    "    medianprops=dict(color='black'),\n",
    "    whiskerprops=dict(color='black'),\n",
    "    capprops=dict(color='black'),\n",
    "    flierprops=dict(markeredgecolor='black'),\n",
    "    patch_artist=True\n",
    ")\n",
    "for i in range(RPVAL_DF_OBS.shape[0]):\n",
    "   plt.plot(i, RPVAL_DF_OBS['RF'][i], 'ro', markersize=10)\n",
    "    \n",
    "plt.title(\"1-Hour Rainfall Depths (Site - Shawbury,Pre-bias correction)\",weight='bold')\n",
    "plt.xlabel(\"Return Period (RP) in Year\",weight='bold')\n",
    "plt.ylabel(\"Rainfall depth (mm)\",weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
